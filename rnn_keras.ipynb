{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from pickle import load\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99993, 10, 62)\n"
     ]
    }
   ],
   "source": [
    "data = open('shakespear.txt', 'r').read()\n",
    "data_out = data[1:]+data[0]\n",
    "chars = sorted(list(set(data)))\n",
    "char2num = {ch:i for i,ch in enumerate(chars)}\n",
    "num2char = {i:ch for i,ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "input_one_hot = np.zeros([len(data), len(chars)])\n",
    "for i in range(len(data)):\n",
    "    input_one_hot[i,char2num[data[i]]] =1\n",
    "\n",
    "\n",
    "\n",
    "seq_length = 10\n",
    "batch_size = len(data)\n",
    "hidden_size = 20\n",
    "TRAINING_ITERS = 1\n",
    "epoch = 1\n",
    "TRAINING_ITERS = 1\n",
    "def generate_data():\n",
    "    pos = np.random.choice(range(seq_length))\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for _ in range(batch_size):\n",
    "        if pos+seq_length+1 >= len(input_one_hot):\n",
    "            pos = np.random.choice(range(seq_length)) # go back to random starting point\n",
    "        seq_x = input_one_hot[pos:pos+seq_length]\n",
    "        seq_y = input_one_hot[pos+seq_length]\n",
    "        batch_x.append(seq_x)\n",
    "        batch_y.append(seq_y)\n",
    "        pos += 1\n",
    "    batch_x = np.array(batch_x)\n",
    "    batch_y = np.array(batch_y)\n",
    "    return batch_x,batch_y\n",
    "    #print(batch_x.shape)\n",
    "X,Y = generate_data()\n",
    "print(X.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 75)                41400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 62)                4712      \n",
      "=================================================================\n",
      "Total params: 46,112\n",
      "Trainable params: 46,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "loss_fn = lambda y_true, y_pred: tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss=loss_fn, optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99993, 10, 62) (99993, 62)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50\n",
      " - 35s - loss: 1.4227 - acc: 0.5668\n",
      "Epoch 26/50\n",
      " - 33s - loss: 1.4129 - acc: 0.5687\n",
      "Epoch 27/50\n",
      " - 33s - loss: 1.4032 - acc: 0.5725\n",
      "Epoch 28/50\n",
      " - 34s - loss: 1.3959 - acc: 0.5744\n",
      "Epoch 29/50\n",
      " - 33s - loss: 1.3877 - acc: 0.5760\n",
      "Epoch 30/50\n",
      " - 34s - loss: 1.3792 - acc: 0.5791\n",
      "Epoch 31/50\n",
      " - 34s - loss: 1.3714 - acc: 0.5806\n",
      "Epoch 32/50\n",
      " - 32s - loss: 1.3641 - acc: 0.5834\n",
      "Epoch 33/50\n",
      " - 34s - loss: 1.3584 - acc: 0.5837\n",
      "Epoch 34/50\n",
      " - 34s - loss: 1.3517 - acc: 0.5863\n",
      "Epoch 35/50\n",
      " - 33s - loss: 1.3448 - acc: 0.5871\n",
      "Epoch 36/50\n",
      " - 33s - loss: 1.3381 - acc: 0.5895\n",
      "Epoch 37/50\n",
      " - 33s - loss: 1.3314 - acc: 0.5907\n",
      "Epoch 38/50\n",
      " - 34s - loss: 1.3261 - acc: 0.5923\n",
      "Epoch 39/50\n",
      " - 33s - loss: 1.3202 - acc: 0.5943\n",
      "Epoch 40/50\n",
      " - 34s - loss: 1.3142 - acc: 0.5964\n",
      "Epoch 41/50\n",
      " - 33s - loss: 1.3098 - acc: 0.5973\n",
      "Epoch 42/50\n",
      " - 32s - loss: 1.3036 - acc: 0.5994\n",
      "Epoch 43/50\n",
      " - 33s - loss: 1.2980 - acc: 0.6013\n",
      "Epoch 44/50\n",
      " - 32s - loss: 1.2938 - acc: 0.6008\n",
      "Epoch 45/50\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, epochs=50, verbose=2,initial_epoch=24)\n",
    "\n",
    "#for i in range(TRAINING_ITERS):\n",
    "#    model.partial_fit(X,Y,verbose=2)\n",
    "\n",
    "# save the model to file\n",
    "#model.save('model.h5')\n",
    "# save the mapping\n",
    "#dump(mapping, open('mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-15-1a919625dfe2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-1a919625dfe2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir=path/to/log-directory\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir=path/to/log-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOLPIO:\n",
      "Narry, my!e\n",
      "To make thy offenve and hunver'st dray I much live\n",
      "O, that in down like a letst thou hast ither by as greature, ta bear,\n",
      "He, I will seaw\n",
      "We preet as life; and, and ream ublesing make oye\n",
      "In my bear;\n",
      "To epalors and he's a mistragagf lord, fixed;\n",
      "And you muty wrate me hy dow\n",
      "To.\n",
      "\n",
      "CRINCLANL:\n",
      "Deke but seen his same: the here, and doing and her faliat before\n",
      "That I shall be bant:\n",
      "And\n",
      "they must;\n",
      "Fear him full more takest that heard Sirnon that ever yet went is vilyare: shall nome the most are her:\n",
      "Of knew poor dreven.\n",
      "\n",
      "TROILUS:\n",
      "A can you both\n",
      "And heard fear-king with wixe and sid to Ruched his hin,\n",
      "And I so; Ant Illake with thouns him the paberest subjectsoance, in then flower-bairs,\n",
      "Indle see with thee, I must deer nider to beds inderun,\n",
      "Who\n",
      "OR I do them from the fallent!\n",
      "\n",
      "ENY SIOLO:\n",
      "I my, why not Tribut!\n",
      "What it that thou not band given mefork to deer me of her begnest from the abjedit to thee, thrue yot wroty.\n",
      "\n",
      "PISARUS:\n",
      "I\n",
      "beseevery therefile oid twalses,\n",
      "And thou point,\n",
      "And are I have Jultit!\n",
      "\n",
      "VONES:\n",
      "And then I was did?\n",
      "\n",
      "SeALANT:\n",
      "And, out that shall he will no: O't wholk!\n",
      "\n",
      "SICINCUS:\n",
      "Then, and makes it catt; and the causin?t we ought;\n",
      "For thou set. I was a mer ancetain:\n",
      "But he was us villat's may, lodgest\n",
      "That engrous a meart I shall be unlieft firror me Marter, it shamam a meer in the mothalm, force out from prais heraw.\n",
      "\n",
      "PENES:\n",
      "Bind alics be pased.\n",
      "\n",
      "ANNE PGELO:\n",
      "Sail thing, ladets, and the all cusp perfith a nour and beasbonon of kin.\n",
      "\n",
      "GONARO:\n",
      "Or, I whoughts and to miderous you so.\n",
      "\n",
      "PRINCE TERALFO:\n",
      "Chowsex's olfinter as good a mish sent think and great lie her.\n",
      "\n",
      "HASSINCAN:\n",
      "Our day\n",
      "That hewleres. Here's proted as me? Liver had arpeabtice!\n",
      "\n",
      "Thild well you: and lungerar pour himberch than every spoled us;\n",
      "And me tay that think what truly, and all adgar the joy; are I cannot.\n",
      " not martal'd makes you are lost upain ly ptorive,\n",
      "What meldied her, I do be and subjeccedoref that hold's speak\n",
      "He hours ever queen, that groosticl, come, I'll pragebured and her peat beffently.\n",
      "\n",
      "ANDELIO:\n",
      "Hamk's and some child and sp reast not anceeds, thoughtly pasted and confum with him,\n",
      "Ture the prevence would not repuspcacte of Henter to partets a ourte that own outlack,\n",
      "Why cain valion\n",
      "The souly their be\n",
      "but cicle prance chathers,\n",
      "What is as liesure\n",
      "There it eyes: I pows you that in and, and that you may the seass from your see!\n",
      "But spear, miles of Solfurly tell me, veay him and\n",
      "That them but your heart, and knagean hand, and\n",
      "gains,\n",
      "But nevime merdion with meeber beforew of held, which Jities, and cloar your cornteeny is and coornd:\n",
      "Here shoundersed their drever hand and fortanines but here shall procked our men too arms,\n",
      "Of grace:\n",
      "'Till serul, Sir with rank's montin;\n",
      "Do acko thought for our bunys, if I may shomeardy'ced.\n",
      "\n",
      "Shaso:\n",
      "I think muder's\n",
      "To fire every sevely, and they do Whiline, tull,\n",
      "Comef him.\n",
      "\n",
      "NORFOOLA:\n",
      "I was her me?\n",
      "\n",
      "SewFOUTO:\n",
      "She masters, so whilh be come\n",
      "king\n",
      "my loods his brother was his pall'd by him from dudlort Powted, that sin, 'his true anowere\n",
      "He heart'd behose vinemance;\n",
      "and this is the hised,\n",
      "We holast it, breaked,\n",
      "But II helaked legge would alk,\n",
      "Send put here strect seep to knew in I hearfugh\n",
      "Throight well your grater,\n",
      "Allose preped, and thee, me, me conquents:\n",
      "Thy eye aloon:\n",
      "Mary mast respece: my lord, and all make my hands\n",
      "To are to prop ourniped to Thee, this' gued,-might begin, it witherse\n",
      "Aghast poor indusl:\n",
      "But I will heard in the at\n",
      "That tumbsantect,\n",
      "And beet perforst for heard\n",
      "And marry;\n",
      "Remaid hervighted king\n",
      "He make no, we may ented our seck is shall,\n",
      "With in the seak weaw,\n",
      "And please a mine.\n",
      "\n",
      "PIANCUS:\n",
      "What's wrenk to know.\n",
      "\n",
      "FALETMART:\n",
      "Be withiury crupt and graces\n",
      "And doin or the meed as all the veillances'd in his bloody in disie. But I'll seer mantues mear death,\n",
      "Say werch'd in fair, and fous then ad Master: for sold before it are that intalt\n",
      "nown his sword,\n",
      "here's aptoulce within os aboutly, lebton:\n",
      "There's speaking hold orr:\n",
      "Well, us a am it east that rud; and, bronked to this quicht: news fese promuse on thee. 'Tis at that he hot. Perish turning of fall and confumin Tirrun if, more lowny\n",
      "I do beliesed a proses backs upon clought obearer again thoush youraber that way\n",
      "Have lie vants?\n",
      "\n",
      "HiClINCE:\n",
      "Duth creven:\n",
      "I am her half it speak the pacurest murded;\n",
      "And ouds at her wands the might fallow to sweet wa'st\n",
      "Remouth, Letwelly,\n",
      "What say's red lugghmorse's iticlace,\n",
      "As he amoust procksest me but a strukes and me must father?\n",
      "\n",
      "SecoAndO:\n",
      "But yout all him tell my loods,\n",
      "For she, him he here be no saluny\n",
      "That.\n",
      "\n",
      "SCANCIUS:\n",
      "Thou load, a good grace up have neese flect can this sare and fallen, and Someath you\n",
      "he's the abretts all hath take the requirer caughter's man\n",
      "As live him.\n",
      "\n",
      "ENULD MOCAENA:\n",
      "I'll so much turns appise.\n",
      "'Till bear their ways of Cassinicallens ears:\n",
      "Fales, somen, thy it it, sweetio, our loresssed her precexas a vaiger\n",
      "Mad\n",
      "Ay that I dish blard,\n",
      "That doth. Naw, that he'll follax loss, out are peasice, and as spach to ruth clitures and to\n",
      "Out: and with the purchinger, I was a seaply office with a most exa mant enferand Pare of his bloody heavend trum out than when nof her canst not a putcuse of it secrent me him too, Citrue and fice: to Antenter\n",
      "Up the seader's stain.\n",
      "\n",
      "BENTRY:\n",
      "And not every poare,\n",
      "What'st him\n",
      "At innote\n",
      "Ather, murders my suurrently necesolamending of two speeply fellome in ances.\n",
      "\n",
      "KING JEUNY:\n",
      "Yourur exasure. I cannot ming than I spreak upon my hose of our stancem'd all our eveny Naring thut deen, and know\n",
      "End lied, if uncus me, if hery his ere merge fallablesed, my lord,\n",
      "By's a brows.\n",
      "\n",
      "IOWOL:\n",
      "But not thear heaven grace.\n",
      "\n",
      "MLONCEOTDEBUT:\n",
      "There peartain up more me tfell'st of a villare!\n",
      "\n",
      "Hart seven.\n",
      "\n",
      "EDGARIO:\n",
      "That wear a creation somethers: I shall to known to bear\n",
      "and entertainese\n",
      "Cacciss of, Lord.\n",
      "\n",
      "ROSANINI:\n",
      "Can spowiret to the chimpers of my prot my forture in like's deputite takes ancesc ane dea, is knowh if a brithing this heavens hearw? me party than more in without cirncely.\n",
      "\n",
      "Thery:\n",
      "Groke Ell: Le us them futlese pitties are: been my sightar:\n",
      "As I liver deladly come but to and and sayeamendition:\n",
      "This caffinens;\n",
      "And thou swear'd all wagets you may all thate thy hatet noar then I wo led weqpier they mucwied with thou hearrane would Angerved.\n",
      "\n",
      "DOLETRAME:\n",
      "Here's knitpen and protestect beward,\n",
      "OrCle, as I willon's stains.\n",
      "\n",
      "Becormatalban, good make the scapter's, and from pravents enothing goulm suffer he heres I present me\n",
      "Didmunese my llody death'd thel's tike respecitais at us.\n",
      "\n",
      "Kn any seeft oven\n",
      "To distrardien every traims, bay raghold and knoweds my lord us\n",
      "nith Marger;\n",
      "Our great;\n",
      "Who's I seem's: therefouth.'\n",
      "Oy, Toldite resent,\n",
      "When fege,\n",
      "And not die to be mery men, and this gove\n",
      "to leed, whoughts neam. It sheme\n",
      "whuld Forthrage.\n",
      "\n",
      "LUCED:\n",
      "Ne, 'sis, and the rroff-gneble.\n",
      "But marty sign.\n",
      "\n",
      "SIMENGER:\n",
      "By a knowf fortunes,\n",
      "Sir, you thoulds: what hath doth,\n",
      "But on in thrue gold fice as as well be my bengeved;\n",
      "Ald for them and all chack-dod\n",
      "The kilds and which\n",
      "good by my does!\n",
      "\n",
      "DUKE OF YORK:\n",
      "He, sir, but speens and exseflen, have his foot, and contlach,\n",
      "Ad Seevry:\n",
      "A hancy hath to see, which cill within would in even this defund thou lakes us\n",
      "mens owers' love: but have I goodless from the\n",
      "have some to being cauled your part me were as stains me know as in by miluse\n",
      "Tale ustrects I have anvesous mother:\n",
      "he southering munders.\n",
      "Yourly, as I desracitedsain and give ere upon your firth,\n",
      "Withight not rays\n",
      "With love me is haph no maces:\n",
      "Sir, you manes to see amon\n",
      "I shall hoursh of all the disty\n",
      "Af a vorsout of poevently have so\n",
      "Qiutters yet to make they showers I swear; I am by hamoul-bet was\n",
      "By men in my reasush and here\n",
      "Of speak hath hemple allow I desent to again,-\n",
      "Efine now have solew be uncurman:\n",
      "But men here would not let her shorless\n",
      "at pour, by thise every anote\n",
      "Hach and me\n",
      "Conved unall'et pearty mine seeply we be neted:\n",
      "I would he best conn oud for I reselvem'd hear, I Guity Antoty, and\n",
      "Hear the perforst, it is command thee, and must, with his face,\n",
      "When arry feared bot tern, and heaves morlatly sis:\n",
      "With spert: I'll happish\n",
      "That it grue us and his purfolt, and before you loven! where 'echers a glar more I will I davered to hear you all appervaily me these and fierd,\n",
      "Who marrugh, I have genian about I pray,\n",
      "If the didst,\n",
      "And by dedingacr,\n",
      "But the pale, and in the love. In my deed, Palish\n",
      "And beel me, if she races her call as grace up strait, and could mest me! a kmed beed, his charw, and cromatet;\n",
      "And where her cray 'wlose cain's dised, and man being contuses.\n",
      "Thiskers, and I heart a holle,\n",
      "She will singer ort offer of your itder wanton and live, which year;\n",
      "When graves and time aties on an discourse proud, and sigh;\n",
      "As I will beind thou heart, call back my sour madamstiem how and so drunger with him, marry:\n",
      "'Till are but constice things and against kind thee ently, and thinker Yow me;\n",
      "Herm not!\n",
      "\n",
      "AMHELLAV:\n",
      "I was tell pour take of eaper to bearte these in all part poisured and rank'd go fur live up the partance disposs an heed we speaked our couters:\n",
      "If tiry spame\n",
      "And me must fillly as not, I purd it sink in grace within thee, 'finding\n",
      "Hear your gion light:\n",
      "And where arl seep, as I vele.\n",
      "\n",
      "yeB horld upon,\n",
      "Our lours to more beak\n",
      "And process villain,\n",
      "My lord, at man!\n",
      "\n",
      "SINNO:\n",
      "Goy come and 'Ap shall not a wound\n",
      "yet is the charazed:\n",
      "But it necers hath begon's that alm demertas,\n",
      "With so prupuse\n",
      "Titl, as of suc.\n",
      "Liment me but any heers':\n",
      "Of gliel to day to thy fouly\n",
      "as Ishill a kin\n",
      "cout is not and femeed that fair falled here, thou so,\n",
      "And all the puirs,\n",
      "To deach the place; so sim, my lardly ever; and you, and fronder in villames our his sholds,\n",
      "Stay: I have ast thou cur to diend thee, by me, me, als?\n",
      "Renom cit, led the permy wear their?\n",
      "\n",
      "TOBATUS:\n",
      "of well of you and all the veliag did down, and\n",
      "That have my for halt be kengle\n",
      "Will bringted to the fallatures\n",
      "With her offenced.\n",
      "\n",
      "EBHAQUS:\n",
      "Nay, I'll speak,\n",
      "For all the playert this cinnor fegh'n esom't:\n",
      "Nornath, if thy entmenon I'll sterve for here then;\n",
      "And maidseless'll me, and think offorct,\n",
      "Can upon, as\n",
      "This negvents to mornat oud,\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#generate a sequence of characters with a language model\n",
    "def generate_seq(model, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [char2num[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        encoded = to_categorical(encoded, num_classes=vocab_size)\n",
    "\n",
    "        \n",
    "        # reverse map integer to character\n",
    "        #out_char = ''\n",
    "        if in_text[-1] == ' ' or in_text[-1] =='\\n' or in_text[-1]==':':\n",
    "        #if True:\n",
    "            probs = model.predict_proba(encoded, verbose=0)\n",
    "            yhat = np.random.choice(Y.shape[1],p=probs[0])\n",
    "            #print(yhat)\n",
    "            char = num2char[yhat]\n",
    "            in_text += char\n",
    "        else:\n",
    "            yhat = model.predict_classes(encoded,verbose=0)\n",
    "            #print(yhat)\n",
    "            char = num2char[yhat[0]]\n",
    "            in_text += char   \n",
    "    return in_text\n",
    "\n",
    "\n",
    "print(generate_seq(model, 10, 'T', 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.55516655e-12 3.43545293e-03 4.37857561e-05 2.95649306e-03\n",
      " 1.32304398e-04 1.32646455e-05 2.34582811e-04 5.97190319e-06\n",
      " 1.77629539e-04 2.36619017e-05 3.37657667e-07 2.80047118e-07\n",
      " 3.12416604e-09 2.98170448e-08 3.29765228e-08 7.28715421e-11\n",
      " 3.31476201e-11 1.75440988e-07 7.68207045e-08 9.40990503e-07\n",
      " 3.78405751e-09 6.86346224e-09 1.07870992e-08 1.10448298e-07\n",
      " 5.05748261e-08 3.73938008e-07 3.63012009e-10 4.71070400e-08\n",
      " 5.50160166e-06 4.13635632e-08 1.73824366e-09 5.66055064e-07\n",
      " 1.11106101e-06 8.68983534e-07 1.72449120e-07 1.71315033e-08\n",
      " 3.19471210e-03 3.94385324e-06 3.95673960e-05 7.38599777e-01\n",
      " 3.22828069e-03 1.20052464e-04 2.15253181e-06 8.39210674e-03\n",
      " 8.00172682e-04 4.14027738e-07 7.28923408e-03 9.18784663e-02\n",
      " 3.03084757e-02 1.09101973e-04 2.78693711e-04 8.95698031e-05\n",
      " 8.26176063e-07 2.70100918e-05 9.33347717e-02 1.23015570e-03\n",
      " 6.71466161e-03 1.36334875e-05 1.36025716e-04 1.75431527e-08\n",
      " 7.17429956e-03 5.03732542e-11]\n",
      "dict_items([('\\n', 0), (' ', 1), ('!', 2), (\"'\", 3), (',', 4), ('-', 5), ('.', 6), (':', 7), (';', 8), ('?', 9), ('A', 10), ('B', 11), ('C', 12), ('D', 13), ('E', 14), ('F', 15), ('G', 16), ('H', 17), ('I', 18), ('J', 19), ('K', 20), ('L', 21), ('M', 22), ('N', 23), ('O', 24), ('P', 25), ('Q', 26), ('R', 27), ('S', 28), ('T', 29), ('U', 30), ('V', 31), ('W', 32), ('X', 33), ('Y', 34), ('Z', 35), ('a', 36), ('b', 37), ('c', 38), ('d', 39), ('e', 40), ('f', 41), ('g', 42), ('h', 43), ('i', 44), ('j', 45), ('k', 46), ('l', 47), ('m', 48), ('n', 49), ('o', 50), ('p', 51), ('q', 52), ('r', 53), ('s', 54), ('t', 55), ('u', 56), ('v', 57), ('w', 58), ('x', 59), ('y', 60), ('z', 61)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "in_text = 'hello worl'\n",
    "encoded = [mapping[char] for char in in_text]\n",
    "encoded = pad_sequences([encoded], maxlen=10, truncating='pre')\n",
    "encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "yhat = model.predict_proba(encoded, verbose=0)\n",
    "print(yhat[0])\n",
    "print(mapping.items())\n",
    "np.random.choice(y.shape[1],p=yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 62)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 75)                41400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                4712      \n",
      "=================================================================\n",
      "Total params: 46,112\n",
      "Trainable params: 46,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(75, input_shape=(batch_x.shape[1], batch_x.shape[2])))\n",
    "model.add(Dense(vocab_size, input_shape = (50,20), activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (2, 50, 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-dac1ceddcaf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    905\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape (2, 50, 62)"
     ]
    }
   ],
   "source": [
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(batch_x, batch_y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
